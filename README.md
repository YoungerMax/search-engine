# üîé Distributed Search Engine Crawler

Build a **production-style, horizontally scalable** crawler + indexing pipeline with Python, Postgres, batch jobs, and a FastAPI search API.

---

## ‚ú® Why this project?

- ‚ö° **Fast stateless workers** for crawl throughput.
- üß† **Offline global computation** (PageRank, link resolution, BM25 stats).
- üóÑÔ∏è **Migration-first schema management** with Alembic.
- üê≥ **Docker Compose + Swarm friendly** deployment workflow.

---

## üß± Architecture at a glance

```text
Crawler Workers (append-heavy writes)
        ‚Üì
      Postgres
        ‚Üì
Batch Jobs (duplicate detection, link graph, pagerank, bm25)
        ‚Üì
     Search API
```

---

## üöÄ Quick Start

### 1) Configure environment

This repo ships with a baseline `.env`. Update values as needed:

- `POSTGRES_USER`
- `POSTGRES_PASSWORD`
- `POSTGRES_DB`
- `CRAWLER_USER_AGENT`
- `QUEUE_BATCH_SIZE`
- `CRAWLER_CONCURRENCY`
- `REQUEST_TIMEOUT_S`
- `BATCH_INTERVAL_S`

### 2) Start the stack

```bash
docker compose up --build
```

### 3) Query search API

```bash
curl 'http://localhost:8000/search?q=example'
curl 'http://localhost:8000/search?q=example&limit=10&offset=0'
```

### Seed a URL into the crawl queue

```bash
python scripts/seed_url.py 'https://example.com'
```

---

## üîå API

### `GET /search`

Query parameters:

- `q` (required): search query text
- `limit` (optional, default 20, max 100)
- `offset` (optional, default 0)

Response shape:

```json
{
  "results": [
    {
      "title": "...",
      "description": "...",
      "url": "https://...",
      "score": 1.234
    }
  ]
}
```

---

## üß© Components

- `app/crawler/queue_manager.py`: queue transitions + batched dequeue.
- `app/crawler/worker.py`: fetch ‚Üí parse ‚Üí validate ‚Üí tokenize ‚Üí persist.
- `app/batch/*.py`: offline global jobs.
- `app/batch/runner.py`: always-running batch scheduler loop.
- `app/api/main.py`: FastAPI `/search` endpoint.
- `alembic/`: versioned migrations (single schema source of truth).
- `scripts/update_cluster.sh`: Swarm update with migration window.

---

## üêù Swarm Deploy / Update

Builds from `main` publish a multi-arch image (`linux/amd64` and `linux/arm64`) to `ghcr.io/youngermax/search-engine:latest` via GitHub Actions.
Swarm deploys pull that image directly from GHCR on either architecture.

```bash
docker swarm init
./scripts/update_cluster.sh
```

Update flow:

1. Deploy stack definition.
2. Scale migrator + crawler/API/batch down.
3. Wait for Postgres to be running.
4. Run migrator with retries (handles Postgres startup windows).
5. Scale crawler/API/batch back up.

---

## ‚è±Ô∏è Batch cadence

`batch-jobs` runs continuously and executes the full pipeline every `BATCH_INTERVAL_S` seconds.
